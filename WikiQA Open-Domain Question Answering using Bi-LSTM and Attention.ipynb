{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 2023 CITS4012 Assignment"],"metadata":{"id":"32yCsRUo8H33"}},{"cell_type":"markdown","source":["# Readme\n","\n","#### **Group 31**\n","- Abhishek Anand (23598144)\n","- Shaikh Enamul Haque (23440037)\n","- Jia Min Ho (23337561)\n","\n","\n","In this project, we are implementing a Wiki QA (Question Answering) framework using the Sequence model and different NLP features. The QA framework has the ability to read documents/texts and answer questions about them.\n","\n","\n","**Load and Evaluation of Model**\n","\n","To load the provided model and evaluate it on test data, below commands need to be performed.\n","\n","Apart from the below steps, it is also essential that all the code blocks in DataSet Processing and QA Model Implementation sections are executed upfront.\n","\n","The Dataset Processing section takes care of data wrangling. QA Model Implementation section takes care of building all word vector representations which are needed for the loaded document and question models.\n","\n","The below command will print out the precision, recall and f1 score achieved by the model on the test dataset.\n","\n","```bash\n","model_document = torch.load('document_model.pt')\n","model_document.eval()\n","\n","question_model = torch.load('question_model.pt')\n","question_model.eval()\n","\n","precision, recall, f1 = evaluate_model(model_document, question_model)\n","```"],"metadata":{"id":"XCybYoGz8YWQ"}},{"cell_type":"markdown","source":["# 1.DataSet Processing"],"metadata":{"id":"6po98qVA8bJD"}},{"cell_type":"markdown","source":["Set up the necessary dependencies, downloads training and testing files from Google Drive, and loads them into pandas DataFrames. These files ('WikiQA-train.tsv' and 'WikiQA-test.tsv') contains sentences, questions and answer lebels for further analysis."],"metadata":{"id":"M4OGR43RSrIh"}},{"cell_type":"code","source":["# Code to download file into Colaboratory:\n","!pip install -U -q PyDrive\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","# Authenticate and create the PyDrive client.\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)\n","\n","import pandas as pd\n","import re\n","import nltk\n","nltk.download('punkt')\n","from nltk.tokenize import word_tokenize\n","\n","# Read the training file\n","id = '1SXoGbD9WZHwhpqR-cBw7-8_7Ri06nIb6'\n","downloaded = drive.CreateFile({'id':id}) \n","downloaded.GetContentFile('WikiQA-train.tsv')  \n","wiki_train_df = pd.read_table('WikiQA-train.tsv')\n","\n","# Read the test file\n","id = '1TwuDSxlcAFDnTRpF-GRvqRXoR_UsJznH'\n","downloaded = drive.CreateFile({'id':id}) \n","downloaded.GetContentFile('WikiQA-test.tsv')  \n","wiki_test_df = pd.read_table('WikiQA-test.tsv')"],"metadata":{"id":"qvff21Hv8zjk","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3598fe53-33c3-43e6-d86e-a9b5051b8ca1","executionInfo":{"status":"ok","timestamp":1684283223161,"user_tz":-480,"elapsed":40317,"user":{"displayName":"sk enamul haque","userId":"15013482814543587646"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]}]},{"cell_type":"markdown","source":["### Data Wrangling"],"metadata":{"id":"wafHjqIDZ8yU"}},{"cell_type":"markdown","source":["Process the question and sentence data from the DataFrame, tokenizes the sentences, and assigns appropriate labels to the words based on their position relative to the answers. The processed data is then stored in separate lists for both training and test sets. We have defined 4 lebels here \"BA\" - Before Answer, \"A\" - Answer, \"AA\" - After Answer and \"PAD\"."],"metadata":{"id":"d_zAM6N1SvuP"}},{"cell_type":"code","source":["# BA - Before Answer, A - Answer, AA - After Answer\n","token_labels = ['[BA]', '[A]', '[AA]', '[PAD]']\n","\n","# Build the Training and Test Question, Document and Document Labels\n","def construct_questions_documents_labels(wiki_df, is_test_data):\n","  unique_question_ids = wiki_df['QuestionID'].unique()\n","\n","  questions = []\n","  documents = []\n","  document_sentences = []\n","  document_labels = []\n","  document_answers = []\n","\n","  for one_question_id in unique_question_ids:\n","    matching_indices = wiki_df.index[wiki_df['QuestionID'] == one_question_id].tolist()\n","    \n","    question = wiki_df.loc[matching_indices[0]]['Question']\n","    # Remove all punctuations\n","    question = re.sub(r'[^\\w\\s]','', question)\n","\n","    one_document = []\n","    one_document_sentences = ''\n","    one_document_labels = []\n","    one_document_answer = []\n","    answer_label = ''\n","    answer_covered = False\n","\n","    for matching_index in matching_indices:\n","      one_sentence = wiki_df.loc[matching_index]['Sentence']\n","\n","      # If the sentence does not end with a dot, then add the dot to it.\n","      if one_sentence.endswith('.'):\n","        one_document_sentences += one_sentence + \" \"\n","      else:\n","        one_document_sentences += one_sentence + \". \"\n","      \n","      # Get the words of the sentence after removing all punctuations\n","      one_sentence_words = word_tokenize(re.sub(r'[^\\w\\s]','', one_sentence))\n","\n","      # Form the document labels - before answer will be BA, answer will be A, after answer will be AA, and PAD\n","      answer_label = wiki_df.loc[matching_index]['Label']\n","\n","      if answer_label == 1:\n","        one_document_answer.append(one_sentence)\n","        answer_covered = True\n","        word_counter = 0\n","\n","        for oneWord in one_sentence_words:\n","          one_document.append(oneWord)\n","          if word_counter == 0:\n","            one_document_labels.append(token_labels.index('[A]'))\n","          elif word_counter == len(one_sentence_words) - 1:\n","            one_document_labels.append(token_labels.index('[A]'))\n","          else:\n","            one_document_labels.append(token_labels.index('[A]'))\n","          word_counter += 1\n","      else:\n","        if answer_covered == True:\n","          # Answer is covered, so we put all tags as After Answer\n","          for oneWord in one_sentence_words:\n","            one_document.append(oneWord)\n","            one_document_labels.append(token_labels.index('[AA]'))\n","        else:\n","          for oneWord in one_sentence_words:\n","            one_document.append(oneWord)\n","            one_document_labels.append(token_labels.index('[BA]'))\n","\n","    # For training data, add only those documents which have answer. But for test, add all the documents to the list\n","    if answer_covered or is_test_data:\n","      questions.append(question)\n","      documents.append(one_document)\n","      document_sentences.append(one_document_sentences)\n","      document_labels.append(one_document_labels)\n","      document_answers.append(one_document_answer)\n","\n","  return questions, documents, document_sentences, document_labels, document_answers\n","\n","\n","# train_questions - List of questions. Each element is a list of words in the question.\n","# train_documents - List of documents. Each element is a list of sentences in the document.\n","# train_document_sentences - List of documents. Each element is a string of all sentences in the document.\n","# train_document_labels - List of document labels. Each element is a list of labels for each word in the document.\n","# train_document_answers - List of answers for the questions. This will be used only during model evaluation to compute precision, recall etc.\n","train_questions, train_documents, train_document_sentences, train_document_labels, train_document_answers = construct_questions_documents_labels(wiki_train_df, False)\n","test_questions, test_documents, test_document_sentences, test_document_labels, test_document_answers = construct_questions_documents_labels(wiki_test_df, True)"],"metadata":{"id":"d_7pSLVX-chO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Calculate the average document length from the training set and identifies the maximum question length. Then check the number of test documents that exceed the average document length. Finally, calculate the total number of training questions."],"metadata":{"id":"mcu74qWgS1MX"}},{"cell_type":"code","source":["# Get the average document length from training set\n","max_document_length = sum(len(sublist) for sublist in train_document_labels) // len(train_document_labels)\n","\n","# Get the maximum question length\n","max_question_length = len(max(train_questions, key=len))\n","\n","# Check how many test documents are greater than max_document_length\n","long_test_documents = [ one_test_document for one_test_document in test_document_labels if len(one_test_document) > max_document_length ]\n","print('There are ' + str(len(long_test_documents)) + ' test documents longer than the average length in training documents. We will let their prediction be affected rather than increase the length of encoder.')\n","\n","no_of_train_questions = len(train_questions)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8QZv5v9ALBOK","outputId":"5bac6efd-88e0-46d3-902b-1a311b9db97e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["There are 249 test documents longer than the average length in training documents. We will let their prediction be affected rather than increase the length of encoder.\n"]}]},{"cell_type":"markdown","source":["# 2.QA Model Implementation"],"metadata":{"id":"1FA2ao2l8hOg"}},{"cell_type":"markdown","source":["### Word Embedding: FastText"],"metadata":{"id":"xdqikTGUaS97"}},{"cell_type":"markdown","source":["Training and testing documents are now merged to create one final document. FastText model is used and trained on the final_document corpus using the Skip_Gram architecture and with few other parameters like vector size, window size, minimum count etc."],"metadata":{"id":"Oapt4hm2S49t"}},{"cell_type":"code","source":["from gensim.models import FastText, Word2Vec\n","\n","# We use 50 dimension word vector representation\n","word_vector_size = 50\n","\n","# To ensure all words have their vector representations, we add train and test documents and use that as corpus\n","all_documents = train_documents + test_documents\n","\n","# Now we initialize and train FastText with Skip Gram architecture (sg=1)\n","# We are using FastText to ensure that POS tags and NER tags also have their vector representations\n","ft_sg_model = FastText(all_documents, vector_size=word_vector_size, window=5, min_count=2, workers=2, sg=1)\n"],"metadata":{"id":"fDJuAMtV5c6C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Feature Extraction"],"metadata":{"id":"f_fDHfi7aenI"}},{"cell_type":"markdown","source":["#### TF-IDF \n","\n","Calculating the TF-IDF and return them as dictionary where the keys are the words, and the values are their corresponding counts. This dictionary is then assigned to the another dictionary with the document ID as the key.\n","\n","This function then used to calculate the TF-IDF score for training and testing documents."],"metadata":{"id":"pwmerBVHS_oL"}},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","import numpy as np\n","from collections import Counter\n","import math\n","\n","\n","def GetTDIDF(document_sentences):\n","  # Calculate Document Frequency first - Number of documents in which a word is present\n","  DF = {}\n","  for one_document in document_sentences:\n","    one_document = re.sub(r'[^\\w\\s]','', one_document)\n","    word_list = word_tokenize(one_document)\n","    \n","    words_in_lower_case = [t.lower() for t in word_list]\n","    for one_word in np.unique(words_in_lower_case):\n","      try:\n","        DF[one_word] +=1\n","      except:\n","        DF[one_word] =1\n","\n","  # Calculate TF-IDF for each word in each document\n","  doc_id = 0\n","  total_num_of_documents = len(document_sentences)\n","  # Dictionary of format { doc_id: { 'word1': tdidf1, 'word2': tdidf2 }}\n","  tf_idf_documents = {}\n","\n","  for one_document in document_sentences:\n","    one_document = re.sub(r'[^\\w\\s]','', one_document)\n","    word_list = word_tokenize(one_document)\n","\n","    words_in_lower_case = [t.lower() for t in word_list]\n","    # Initialise counter for the doc\n","    counter = Counter(words_in_lower_case)\n","    \n","    # Calculate total number of words in the doc\n","    total_num_of_words = len(words_in_lower_case)\n","\n","    tf_idf_document = {}\n","    # Get each unique word in the doc\n","    for one_word in np.unique(words_in_lower_case):\n","      \n","      # Calculate Term Frequency \n","      tf = counter[one_word]/total_num_of_words\n","          \n","      # Calculate Document Frequency\n","      df = DF[one_word]\n","\n","      # Calculate Inverse Document Frequency\n","      idf = math.log(total_num_of_documents/(df+1))+1\n","\n","      # Calculate TF-IDF\n","      tf_idf_document[one_word] = tf*idf\n","\n","    # Store the document in dictionary\n","    tf_idf_documents[doc_id] = tf_idf_document\n","    doc_id += 1\n","  \n","  return tf_idf_documents\n","\n","\n","# Get the TF-IDF for training and test sets\n","train_tf_idf_documents = GetTDIDF(train_document_sentences)\n","test_tf_idf_documents = GetTDIDF(test_document_sentences)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nN4KWMgk-_CV","outputId":"51801136-a2ab-4bd2-99a8-fda70d950972"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}]},{"cell_type":"markdown","source":["#### POS Tag\n","\n","In **GetPOSTags** function we are using the \"averaged_perceptron_tagger\" for POS tagging and save the \"pos_tag\" into a dictionary. where the keys are the words, and the values are their corresponding POS tags. This dictionary is then assigned to the pos_tags_documents dictionary with the document ID as the key.\n","\n","This function is used to get the POS_Tags for both training and testing documents."],"metadata":{"id":"gfHZ288wTFY5"}},{"cell_type":"code","source":["# Import the POS tagger\n","from nltk.tag import pos_tag\n","# download the dependency and resource as required\n","nltk.download('averaged_perceptron_tagger')\n","\n","def GetPOSTags(document_sentences):\n","  # Get their POS tag and store in a dictionary\n","  # Dictionary of format { doc_id: { 'word1': 'tag1', 'word2': 'tag2' }}\n","  pos_tags_documents = {}\n","  doc_id = 0\n","\n","  for one_document in document_sentences:\n","    one_document = re.sub(r'[^\\w\\s]','', one_document)\n","    word_list = word_tokenize(one_document)\n","    pos_tags = pos_tag(word_list)\n","\n","    pos_tags_document = {}\n","    for word, word_pos_tag in pos_tags:\n","      pos_tags_document[word] = word_pos_tag\n","\n","    pos_tags_documents[doc_id] = pos_tags_document\n","    doc_id += 1\n","  return pos_tags_documents\n","\n","\n","# Get the POS tags for training and test sets\n","train_pos_tags_documents = GetPOSTags(train_document_sentences)\n","test_pos_tags_documents = GetPOSTags(test_document_sentences)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rvwWppMi5wue","outputId":"57e5eb04-632f-4a57-e4ef-68f2c73915d4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"]}]},{"cell_type":"markdown","source":["#### NER Tag"],"metadata":{"id":"_1KAgD8Bawp5"}},{"cell_type":"markdown","source":["Similarly in **GetNERTags** we are using similar function as GetPOSTags to get NER tags save it to a dictionary."],"metadata":{"id":"atAYsmBqTLc_"}},{"cell_type":"code","source":["import spacy\n","import en_core_web_sm\n","\n","# loading pre-trained model of NER\n","nlp = en_core_web_sm.load()\n","\n","def GetNERTags(document_sentences):\n","\n","  # Get their Named Entity tag and store in dictionary\n","  # Dictionary of format { doc_id: { 'word1': 'tag1', 'word2': 'tag2' }}\n","  ner_tags_documents = {}\n","  doc_id = 0\n","\n","  for one_document in document_sentences:\n","    one_document = re.sub(r'[^\\w\\s]','', one_document)\n","    ner_one_document = nlp(one_document)\n","\n","    ner_tags_document = {}\n","    for token in ner_one_document:\n","      if token.ent_type_ == '':\n","        ner_tag = 'O'\n","      else:\n","        ner_tag = token.ent_type_\n","      \n","      if token.text in ner_tags_document and ner_tags_document[token.text] == 'O':\n","        ner_tags_document[token.text] = ner_tag\n","      \n","      if token.text not in ner_tags_document:\n","        ner_tags_document[token.text] = ner_tag\n","\n","    ner_tags_documents[doc_id] = ner_tags_document\n","    doc_id += 1\n","  \n","  return ner_tags_documents\n","\n","\n","# Get the NER Tags for training and test sets\n","train_ner_tags_documents = GetNERTags(train_document_sentences)\n","test_ner_tags_documents = GetNERTags(test_document_sentences)\n"],"metadata":{"id":"OzdIgFpN3FWN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We created functions to encode and pad textual data using word vectors from a pre-trained FastText model. It incorporates options for TF-IDF values, POS tags, and NER tags, and ensures consistent vector length for documents and questions."],"metadata":{"id":"e3JuuHqWTTCS"}},{"cell_type":"code","source":["# Form the vector for each word in document (pad to match the max length)\n","def encode_pad_documents(documents, use_tf_idf, tf_idf_documents, use_pos_tags, pos_tags_documents, use_ner_tags, ner_tags_documents, max_length, word_vector_size):\n","  doc_id = 0\n","  encoded_documents = []\n","\n","  for one_document in documents:\n","    encoded_one_document = []\n","    one_document = re.sub(r'[^\\w\\s]','', one_document)\n","    word_list = word_tokenize(one_document)\n","\n","    word_counter = 0\n","    for one_word in word_list:\n","      encoded_one_word = []\n","      #word_vec = word_rep.get_vector(word_rep.get_vector(one_word))\n","      encoded_one_word.extend(ft_sg_model.wv[one_word])\n","\n","      # Check if TF-IDF should be added to word vector\n","      if use_tf_idf:\n","        word_tf_idf = tf_idf_documents[doc_id][one_word.lower()]\n","        encoded_one_word.append(word_tf_idf)\n","\n","      # Check if POS Tags  should be added to word vector\n","      if use_pos_tags:\n","        word_pos_tag = pos_tags_documents[doc_id][one_word]\n","        #encoded_one_word.extend(word_rep.get_vector(word_pos_tag))\n","        encoded_one_word.extend(ft_sg_model.wv[word_pos_tag])\n","      \n","      # Check if NER Tags  should be added to word vector\n","      if use_ner_tags:\n","        try:\n","          word_ner_tag = ner_tags_documents[doc_id][one_word]\n","        except:\n","          word_ner_tag = 'O'\n","        #encoded_one_word.extend(word_rep.get_vector(word_ner_tag))\n","        encoded_one_word.extend(ft_sg_model.wv[word_ner_tag])\n","\n","      encoded_one_document.append(encoded_one_word)\n","      word_counter += 1\n","\n","    # Pad it with zero vector\n","    while word_counter < max_length:\n","      pad_vector = [0] * word_vector_size\n","      encoded_one_document.append(pad_vector)\n","      word_counter += 1\n","\n","    # Add encoded document to the list\n","    encoded_documents.append(encoded_one_document)\n","    doc_id += 1\n","\n","  return encoded_documents\n","\n","\n","def pad_document_labels(document_labels, max_length):\n","  padded_document_labels = []\n","\n","  for one_document_label in document_labels:\n","    label_counter = len(one_document_label)\n","    \n","    # Pad it with index of [PAD] token\n","    if label_counter < max_length:\n","      pad_vector = [token_labels.index('[PAD]')] * (max_length - label_counter)\n","      one_document_label.extend(pad_vector)\n","    padded_document_labels.append(one_document_label)\n","\n","  return padded_document_labels\n","\n","\n","def encode_pad_questions(questions, max_length, word_vector_size):\n","  question_id = 0\n","  encoded_questions = []\n","\n","  for one_question in questions:\n","    encoded_one_question = []\n","    one_question = re.sub(r'[^\\w\\s]','', one_question)\n","    word_list = word_tokenize(one_question)\n","\n","    word_counter = 0\n","    for one_word in word_list:\n","      encoded_one_word = []\n","      word_vec = ft_sg_model.wv[one_word]\n","      #word_vec = word_rep.get_vector(one_word)\n","      encoded_one_word.extend(word_vec)\n","\n","      encoded_one_question.append(encoded_one_word)\n","      word_counter += 1\n","\n","    # Pad it with zero vector\n","    while word_counter < max_length:\n","      pad_vector = [0] * word_vector_size\n","      encoded_one_question.append(pad_vector)\n","      word_counter += 1\n","\n","    # Add encoded question to the list\n","    encoded_questions.append(encoded_one_question)\n","    question_id += 1\n","\n","  return encoded_questions\n"],"metadata":{"id":"tIJGVFL9wVzQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### BiLSTMForQuestion\n","\n","Here, we define a BiLSTM (Bidirectional LSTM) model for question processing. The model takes an input with dimensions input_dim and applies a bidirectional LSTM layer. The output includes the LSTM outputs and the concatenated last hidden states from the forward and backward LSTMs."],"metadata":{"id":"Dvk_uhXATZMw"}},{"cell_type":"code","source":["import torch\n","import torch\n","import torch.nn as nn\n","from torch import optim\n","import torch.nn.functional as F\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","class BiLSTMForQuestion(nn.Module):\n","    def __init__(self, input_dim, hidden_dim):\n","\n","      super(BiLSTMForQuestion, self).__init__()\n","\n","      self.input_dim = hidden_dim\n","      self.hidden_dim = hidden_dim\n","      self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True, bidirectional=True)\n","\n","    def forward(self, input):\n","      out, (h_n, _) = self.lstm(input)\n","      \n","      # Concatenate the last hidden states of the forward and backward LSTM\n","      hidden_out = torch.cat((h_n[0, :], h_n[1, :]), dim=0)\n","      \n","      return out, hidden_out\n"],"metadata":{"id":"eEFCaIl2vJ-H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### BiLSTMForDocument\n","\n","Here, we define a BiLSTM (Bidirectional LSTM) model for document processing. The model takes an input with dimensions input_dim and applies a bidirectional LSTM layer. It also includes an attention mechanism that calculates attention weights based on the specified method (e.g., Dot Product, Scaled Dot Product, or Cosine Similarity) between the document tokens and the summary of a question. The final attention output is passed through a fully-connected layer to produce the model's output."],"metadata":{"id":"lHymj8m5TdSP"}},{"cell_type":"code","source":["class BiLSTMForDocument(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, output_dim, num_layers = 1, attention_method = \"Dot Product\"):\n","\n","      super(BiLSTMForDocument, self).__init__()\n","\n","      self.input_dim = input_dim\n","      self.hidden_dim = hidden_dim\n","      self.output_dim = output_dim\n","      self.attention_method = attention_method\n","      self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers, batch_first=True, bidirectional=True)\n","      self.fc = nn.Linear(hidden_dim * 2, output_dim)\n","\n","    def cal_attention(self, document_token, question_summary):\n","      \n","      if self.attention_method == \"Dot Product\":\n","        # document_token of shape [1, 1, 100] & question_summary of shape [1, 100] resulting in [1, 1]\n","        attn_weights = torch.bmm(document_token, question_summary.T.unsqueeze(0))\n","        attn_output = torch.bmm(attn_weights, document_token)\n","        final_output = torch.add(attn_output[0], document_token[0])\n","      \n","      elif self.attention_method == \"Scaled Dot Product\":\n","        attn_weights = torch.bmm(document_token, question_summary.T.unsqueeze(0)) / np.sqrt(question_summary.shape[1])\n","        attn_output = torch.bmm(attn_weights, document_token)\n","        final_output = torch.add(attn_output[0], document_token[0])\n","      \n","      elif self.attention_method == \"Cosine Similarity\":\n","        attn_weights = torch.nn.functional.cosine_similarity(document_token.view(-1, 2 * self.hidden_dim, 1), question_summary.T.unsqueeze(0), dim=1)\n","        attn_output = torch.bmm(attn_weights.view(1, 1, -1), document_token)\n","        final_output = torch.add(attn_output[0], document_token[0])\n","\n","      return final_output\n","\n","    def forward(self, x, question_summary):\n","      out, (h_n, _) = self.lstm(x)\n","      attention_output = self.cal_attention(out, question_summary)\n","      \n","      # Pass the final attention output through the fully-connected layer\n","      out = F.softmax(self.fc(attention_output[0]), dim=0)\n","\n","      return out\n"],"metadata":{"id":"QIEqDDT78q39"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The **asMinutes** function converts the given time in seconds to a string representation in minutes and seconds. The timeSince function calculates the elapsed time since a given starting time (since) and returns a string representation of the elapsed time and the estimated remaining time based on the current progress (percent).\n","\n"],"metadata":{"id":"UWcYQFJKThAY"}},{"cell_type":"code","source":["import time\n","import math\n","\n","# Helper functions for training\n","def asMinutes(s):\n","    m = math.floor(s / 60)\n","    s -= m * 60\n","    return '%dm %ds' % (m, s)\n","\n","def timeSince(since, percent):\n","    now = time.time()\n","    s = now - since\n","    es = s / (percent)\n","    rs = es - s\n","    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"],"metadata":{"id":"p4fH3SAdoTUB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Model Training\n","\n","The **train** function performs training on the given document and question tensors using the provided models, optimizers, and criterion. It calculates the loss based on the predicted document labels and the actual document labels. The gradients are computed and the models are updated using the optimizer.\n","\n","The **trainIters** function iterates over a specified number of training iterations. It randomly selects a document, question, and document labels from the training data. It calls the train function to perform the training on the selected data. It keeps track of the total loss for printing and plotting purposes. It prints the average loss every print_every iterations and appends the average loss to plot_losses every plot_every iterations."],"metadata":{"id":"EkjfC35bT25A"}},{"cell_type":"code","source":["import random\n","\n","def train(document_tensor, question_tensor, document_labels_tensor, model_document, model_question, model_document_optimizer, model_question_optimizer, criterion):\n","\n","    document_length = document_tensor.size(0)\n","    question_length = question_tensor.size(0)\n","    document_labels_length = document_labels_tensor.size(0)\n","\n","    loss = 0      \n","    model_document_optimizer.zero_grad()\n","    model_question_optimizer.zero_grad()\n","\n","\n","    # Get the question summary\n","    _, question_summary = model_question(question_tensor)\n","    \n","    for i in range(document_length):\n","      # Process each token of the document\n","      document_label_output = model_document(document_tensor[i].view(1, 1, -1), question_summary.view(1, 2 * model_question.hidden_dim))\n","\n","      # Compare the predicted token with actual token\n","      loss += criterion(document_label_output.view(1, -1), document_labels_tensor[i].view(1))\n","    \n","    loss.backward()\n","\n","    model_question_optimizer.step()\n","    model_document_optimizer.step()\n","\n","    return loss.item() / document_labels_length\n","\n","\n","def trainIters(model_document, model_question, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n","    start = time.time()\n","    plot_losses = []\n","    print_loss_total = 0  # Reset every print_every\n","    plot_loss_total = 0  # Reset every plot_every\n","    random.seed(1234)\n","\n","    model_document_optimizer = optim.Adam(model_document.parameters(), lr=learning_rate)\n","    model_question_optimizer = optim.Adam(model_question.parameters(), lr=learning_rate)\n","    \n","    criterion = nn.CrossEntropyLoss()\n","\n","    for iter in range(1, n_iters + 1):\n","        random_choice_ix = random.choice(range(no_of_train_questions)) # Get a random index within the scope of input data\n","        document_index_r = encoded_train_documents[random_choice_ix]\n","        question_index_r = encoded_train_questions[random_choice_ix]\n","        document_labels_index_r = encoded_train_document_labels[random_choice_ix]\n","        \n","        document_tensor = torch.FloatTensor(document_index_r).to(device)\n","        question_tensor = torch.FloatTensor(question_index_r).to(device) \n","        document_labels_tensor = torch.LongTensor(document_labels_index_r).to(device)\n","\n","        loss = train(document_tensor, question_tensor, document_labels_tensor, model_document, model_question, model_document_optimizer, model_question_optimizer, criterion)\n","        print_loss_total += loss\n","        plot_loss_total += loss\n","\n","        if iter % print_every == 0:\n","            print_loss_avg = print_loss_total / print_every\n","            print_loss_total = 0\n","            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n","                                         iter, iter / n_iters * 100, print_loss_avg))\n","\n","        if iter % plot_every == 0:\n","            plot_loss_avg = plot_loss_total / plot_every\n","            plot_losses.append(plot_loss_avg)\n","            plot_loss_total = 0\n"],"metadata":{"id":"XudVl5nPnID0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Model Prediction and Evaluation\n","\n","**predict_answer**: Predicts the answer for a given document and question by utilizing trained models. It returns a list of predicted probabilities for the Start-Of-Answer (SOA) tag.\n","\n","**evaluate_model**: Evaluates the performance of the document and question models on a test set. It predicts answers for each test question using the predict_answer function and compares them to the actual answers. The function calculates precision, recall, and F1 score to assess the model's performance."],"metadata":{"id":"nfIgvfYRUKhE"}},{"cell_type":"code","source":["from nltk.tokenize import word_tokenize, sent_tokenize\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import precision_score, recall_score, f1_score\n","\n","\n","def predict_answer(model_document, model_question, encoded_document, encoded_question, document):\n","  with torch.no_grad():\n","    document_tensor = torch.FloatTensor(encoded_document).to(device)\n","    question_tensor = torch.FloatTensor(encoded_question).to(device)\n","    \n","    _, question_summary = model_question(question_tensor)\n","\n","    predicted_soa_probabilities = []\n","\n","    # We need to predict only for the actual length of the document and not for the padded tokens\n","    document_actual_length = len(document)\n","    for i in range(document_actual_length):\n","      document_label_output = model_document(document_tensor[i].view(1, 1, -1), question_summary.view(1, 100))\n","      \n","      # Extract the probability of SOA tag to find the word with maximum probability\n","      predicted_soa_probabilities.append(document_label_output[1].item())\n","      \n","\n","  return predicted_soa_probabilities\n","\n","\n","def evaluate_model(model_document, model_question, print_first_5 = False):\n","\n","  # Evaluate Model on Test set\n","  no_of_questions = len(encoded_test_questions)\n","  #no_of_questions = 50\n","  actual_answer_array = [ 0 ] * no_of_questions\n","  predicted_answer_array = [ 0 ] * no_of_questions\n","  for i in range(no_of_questions):\n","    predicted_soa_probabilities = predict_answer(model_document, model_question, encoded_test_documents[i], encoded_test_questions[i], test_documents[i])\n","\n","    soa_index = predicted_soa_probabilities.index(max(predicted_soa_probabilities))\n","    soa_word = test_documents[i][soa_index]\n","    \n","    # There is an actual answer for the question\n","    if len(test_document_answers[i]) > 0:\n","      actual_answer_array[i] = 1\n","\n","    sentences_in_document = sent_tokenize(test_document_sentences[i])\n","    answer_for_document = []\n","    for one_sentence in sentences_in_document:\n","      clean_sentence = re.sub(r'[^\\w\\s]','', one_sentence)\n","      if soa_word in word_tokenize(clean_sentence):\n","        answer_for_document.append(one_sentence)\n","    \n","    # Predicted Answer matches Actual Answer OR There was no answer for the question but we still predicted answer\n","    if len(set(answer_for_document).intersection(test_document_answers[i])) > 0 or actual_answer_array[i] == 0:\n","      predicted_answer_array[i] = 1\n","    \n","    # Print the actual and predicted answers for the first 5 questions\n","    if print_first_5 and i <= 5:\n","      print(\"Actual Answer:\", test_document_answers[i])\n","      print(\"Predicted Answer:\", answer_for_document)\n","      print(\"\\n\")\n","\n","\n","  precision = precision_score(actual_answer_array, predicted_answer_array)\n","  recall = recall_score(actual_answer_array, predicted_answer_array)\n","  f1 = f1_score(actual_answer_array, predicted_answer_array)\n","\n","  print(\"\\nBelow is the performance metrics of this model on test data:\")\n","  print(\"Precision:\", precision)\n","  print(\"Recall:\", recall)\n","  print(\"F1 score:\", f1)\n","\n","  return precision, recall, f1\n"],"metadata":{"id":"24z1oyfXm_A7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3.Model Testing"],"metadata":{"id":"EzGuzHPE87Ya"}},{"cell_type":"markdown","source":["###3.1. Input Embedding Ablation Study\n","\n","\n"],"metadata":{"id":"cTNGfO0h9I3W"}},{"cell_type":"markdown","source":["This embedding ablation study evaluates the performance of a model using only FastText word embeddings without additional features such as TF-IDF, POS tags, and NER tags. The training and evaluation processes are conducted, and the resulting precision, recall, and F1 scores are recorded in a DataFrame called **performance_df**."],"metadata":{"id":"E08yEJzTUS-E"}},{"cell_type":"code","source":["# We will create a dataframe that will capture performance metrics of all variants we try\n","performance_df = pd.DataFrame(columns=['Model Type', 'Model Description', 'Precision Score', 'Recall Score', 'F1 Score'])\n","\n","# We will first test with only FastText embedding\n","use_tf_idf = False\n","use_pos_tags = False\n","use_ner_tags = False\n","document_word_vector_size = word_vector_size + (word_vector_size * use_pos_tags) + (word_vector_size * use_ner_tags) + use_tf_idf\n","\n","# encoded_train_documents - List of all documents - Each element is a list of size max_document_length. Each element of this list is a vector of size [word_vector_size * X + 1] - X can be 1,2 or 3\n","# padded_train_document_labels - Similar to train_document_labels. Only padded to match the max_document_length\n","# encoded_train_questions - List of all questions - Each element is a list of size max_question_length. Each element of this list is a vector of size [word_vector_size]\n","\n","encoded_train_documents = encode_pad_documents(train_document_sentences, \n","                                               use_tf_idf, train_tf_idf_documents, \n","                                               use_pos_tags, train_pos_tags_documents, \n","                                               use_ner_tags, train_ner_tags_documents, \n","                                               max_document_length, document_word_vector_size)\n","\n","encoded_train_document_labels = pad_document_labels(train_document_labels, max_document_length)\n","encoded_train_questions = encode_pad_questions(train_questions, max_question_length, word_vector_size)\n","\n","encoded_test_documents = encode_pad_documents(test_document_sentences, \n","                                              use_tf_idf, test_tf_idf_documents, \n","                                              use_pos_tags, test_pos_tags_documents, \n","                                              use_ner_tags, test_ner_tags_documents, \n","                                              max_document_length, document_word_vector_size)\n","\n","encoded_test_questions = encode_pad_questions(test_questions, max_question_length, word_vector_size)\n","\n","\n","# Hidden dimension of the Bi-LSTM layer\n","hidden_size = 50\n","\n","# Create the model\n","model_question1 = BiLSTMForQuestion(word_vector_size, hidden_size).to(device)\n","model_document1 = BiLSTMForDocument(document_word_vector_size, hidden_size, len(token_labels)).to(device)\n","\n","no_of_epoch = 1000\n","learning_rate_val = 0.001\n","\n","# Train the model on training data\n","trainIters(model_document1, model_question1, no_of_epoch, print_every=100, learning_rate=learning_rate_val)\n","\n","# Evaluate the model on test data\n","model_document1.eval()\n","model_question1.eval()\n","precision, recall, f1 = evaluate_model(model_document1, model_question1)\n","\n","performance_df.loc[0] = ['Input Embedding Ablation Study', 'FastText only. No TFIDF, POS and NER tags', precision, recall, f1 ]"],"metadata":{"id":"XEVsyvrc9VHL","colab":{"base_uri":"https://localhost:8080/"},"outputId":"108d0f5b-541e-46c8-de5a-dc41aef5ef65"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0m 46s (- 7m 0s) (100 10%) 1.2448\n","1m 33s (- 6m 13s) (200 20%) 1.0818\n","2m 18s (- 5m 22s) (300 30%) 0.9802\n","3m 8s (- 4m 42s) (400 40%) 0.9775\n","3m 50s (- 3m 50s) (500 50%) 0.9903\n","4m 37s (- 3m 5s) (600 60%) 0.9926\n","5m 24s (- 2m 18s) (700 70%) 1.0275\n","6m 13s (- 1m 33s) (800 80%) 0.9608\n","6m 59s (- 0m 46s) (900 90%) 0.9763\n","7m 46s (- 0m 0s) (1000 100%) 1.0016\n","\n","Below is the performance metrics of this model on test data:\n","Precision: 0.17584745762711865\n","Recall: 0.34439834024896265\n","F1 score: 0.23281907433380086\n"]}]},{"cell_type":"markdown","source":["we experiment with including TF-IDF and POS tag embeddings in addition to FastText word embeddings. We evaluate the performance of the model on the test data after training it on the modified training data. The results, including precision, recall, and F1 score, are recorded in the **performance_df** DataFrame. This variant represents an extension of the input embedding ablation study, incorporating more contextual information for document representation."],"metadata":{"id":"kNi79rbNUfpt"}},{"cell_type":"code","source":["# We will now test with all TFIDF and POS Tag embeddings added\n","use_tf_idf = True\n","use_pos_tags = True\n","use_ner_tags = False\n","document_word_vector_size = word_vector_size + (word_vector_size * use_pos_tags) + (word_vector_size * use_ner_tags) + use_tf_idf\n","\n","encoded_train_documents = encode_pad_documents(train_document_sentences, \n","                                               use_tf_idf, train_tf_idf_documents, \n","                                               use_pos_tags, train_pos_tags_documents, \n","                                               use_ner_tags, train_ner_tags_documents, \n","                                               max_document_length, document_word_vector_size)\n","\n","#encoded_train_document_labels = pad_document_labels(train_document_labels, max_document_length)\n","#encoded_train_questions = encode_pad_questions(train_questions, max_question_length, word_vector_size)\n","\n","encoded_test_documents = encode_pad_documents(test_document_sentences, \n","                                              use_tf_idf, test_tf_idf_documents, \n","                                              use_pos_tags, test_pos_tags_documents, \n","                                              use_ner_tags, test_ner_tags_documents, \n","                                              max_document_length, document_word_vector_size)\n","\n","#encoded_test_questions = encode_pad_questions(test_questions, max_question_length, word_vector_size)\n","\n","\n","# Hidden dimension of the Bi-LSTM layer\n","hidden_size = 50\n","\n","# Create the model\n","model_question2 = BiLSTMForQuestion(word_vector_size, hidden_size).to(device)\n","model_document2 = BiLSTMForDocument(document_word_vector_size, hidden_size, len(token_labels)).to(device)\n","\n","# Train the model on training data\n","trainIters(model_document2, model_question2, no_of_epoch, print_every=100, learning_rate=learning_rate_val)\n","\n","# Evaluate the model on test data\n","model_document2.eval()\n","model_question2.eval()\n","precision, recall, f1 = evaluate_model(model_document2, model_question2)\n","\n","performance_df.loc[1] = ['Input Embedding Ablation Study', 'FastText with TFIDF and POS tags. No NER tag', precision, recall, f1 ]"],"metadata":{"id":"XYwHx6LcBp5d","colab":{"base_uri":"https://localhost:8080/"},"outputId":"392a223d-c6d8-46ec-8658-773d5f173ac6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0m 52s (- 7m 53s) (100 10%) 1.2280\n","1m 43s (- 6m 52s) (200 20%) 1.0574\n","2m 37s (- 6m 8s) (300 30%) 0.9793\n","3m 34s (- 5m 21s) (400 40%) 0.9768\n","4m 24s (- 4m 24s) (500 50%) 0.9899\n","5m 20s (- 3m 33s) (600 60%) 0.9920\n","6m 13s (- 2m 40s) (700 70%) 1.0274\n","7m 12s (- 1m 48s) (800 80%) 0.9608\n","8m 7s (- 0m 54s) (900 90%) 0.9759\n","9m 0s (- 0m 0s) (1000 100%) 1.0015\n","\n","Below is the performance metrics of this model on test data:\n","Precision: 0.1670235546038544\n","Recall: 0.3236514522821577\n","F1 score: 0.22033898305084748\n"]}]},{"cell_type":"markdown","source":["we further extend the input embedding ablation study by including TF-IDF, POS tags, and NER tags as additional feature embeddings. We encode and pad the training and test documents with these embeddings and train a model using the modified training data. The performance of the model is evaluated on the test data, and the results are recorded in the **performance_df** DataFrame. This variant explores the impact of incorporating multiple types of contextual information in document representation."],"metadata":{"id":"QPhz2FZtU4w8"}},{"cell_type":"code","source":["# We will now test with all 3 feature embeddings\n","use_tf_idf = True\n","use_pos_tags = True\n","use_ner_tags = True\n","document_word_vector_size = word_vector_size + (word_vector_size * use_pos_tags) + (word_vector_size * use_ner_tags) + use_tf_idf\n","\n","encoded_train_documents = encode_pad_documents(train_document_sentences, \n","                                               use_tf_idf, train_tf_idf_documents, \n","                                               use_pos_tags, train_pos_tags_documents, \n","                                               use_ner_tags, train_ner_tags_documents, \n","                                               max_document_length, document_word_vector_size)\n","\n","#encoded_train_document_labels = pad_document_labels(train_document_labels, max_document_length)\n","#encoded_train_questions = encode_pad_questions(train_questions, max_question_length, word_vector_size)\n","\n","encoded_test_documents = encode_pad_documents(test_document_sentences, \n","                                              use_tf_idf, test_tf_idf_documents, \n","                                              use_pos_tags, test_pos_tags_documents, \n","                                              use_ner_tags, test_ner_tags_documents, \n","                                              max_document_length, document_word_vector_size)\n","\n","#encoded_test_questions = encode_pad_questions(test_questions, max_question_length, word_vector_size)\n","\n","\n","# Hidden dimension of the Bi-LSTM layer\n","hidden_size = 50\n","\n","# Create the model\n","model_question3 = BiLSTMForQuestion(word_vector_size, hidden_size).to(device)\n","model_document3 = BiLSTMForDocument(document_word_vector_size, hidden_size, len(token_labels)).to(device)\n","\n","# Train the model on training data\n","trainIters(model_document3, model_question3, no_of_epoch, print_every=100, learning_rate=learning_rate_val)\n","\n","# Evaluate the model on test data\n","model_document3.eval()\n","model_question3.eval()\n","precision, recall, f1 = evaluate_model(model_document3, model_question3)\n","\n","performance_df.loc[2] = ['Input Embedding Ablation Study', 'FastText with TFIDF, POS and NER tags', precision, recall, f1 ]"],"metadata":{"id":"6ZVeNYIH9IaL","colab":{"base_uri":"https://localhost:8080/"},"outputId":"13cdd360-3a7b-4f45-ee75-e95e59eb53e6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0m 57s (- 8m 38s) (100 10%) 1.2133\n","1m 50s (- 7m 21s) (200 20%) 1.0522\n","2m 46s (- 6m 28s) (300 30%) 0.9786\n","3m 44s (- 5m 36s) (400 40%) 0.9760\n","4m 35s (- 4m 35s) (500 50%) 0.9894\n","5m 28s (- 3m 39s) (600 60%) 0.9905\n","6m 21s (- 2m 43s) (700 70%) 1.0274\n","7m 18s (- 1m 49s) (800 80%) 0.9602\n","8m 10s (- 0m 54s) (900 90%) 0.9746\n","9m 3s (- 0m 0s) (1000 100%) 1.0005\n","\n","Below is the performance metrics of this model on test data:\n","Precision: 0.1487964989059081\n","Recall: 0.2821576763485477\n","F1 score: 0.19484240687679086\n"]}]},{"cell_type":"markdown","source":["###3.2. Attention Ablation Study"],"metadata":{"id":"uX7nFwMo9WBE"}},{"cell_type":"markdown","source":["we continue with the best performing input embedding variant, which is FastText embedding only. We set the flags for TF-IDF, POS tags, and NER tags to **False** to exclude them. We encode and pad the training and test documents using only FastText embeddings. The model architecture consists of a BiLSTM for question processing and a BiLSTM with attention mechanism for document processing. The attention mechanism used is **dot product attention**. The model is trained on the training data and evaluated on the test data. The performance metrics are recorded in the performance_df DataFrame. This variant explores the impact of using dot product attention in the document processing stage."],"metadata":{"id":"h4pjKOJbU-ST"}},{"cell_type":"code","source":["# We will move ahead with only FastText embedding as it was the best performing input embedding variant tried above\n","use_tf_idf = False\n","use_pos_tags = False\n","use_ner_tags = False\n","document_word_vector_size = word_vector_size + (word_vector_size * use_pos_tags) + (word_vector_size * use_ner_tags) + use_tf_idf\n","\n","encoded_train_documents = encode_pad_documents(train_document_sentences, \n","                                               use_tf_idf, train_tf_idf_documents, \n","                                               use_pos_tags, train_pos_tags_documents, \n","                                               use_ner_tags, train_ner_tags_documents, \n","                                               max_document_length, document_word_vector_size)\n","\n","encoded_train_document_labels = pad_document_labels(train_document_labels, max_document_length)\n","encoded_train_questions = encode_pad_questions(train_questions, max_question_length, word_vector_size)\n","\n","encoded_test_documents = encode_pad_documents(test_document_sentences, \n","                                              use_tf_idf, test_tf_idf_documents, \n","                                              use_pos_tags, test_pos_tags_documents, \n","                                              use_ner_tags, test_ner_tags_documents, \n","                                              max_document_length, document_word_vector_size)\n","\n","encoded_test_questions = encode_pad_questions(test_questions, max_question_length, word_vector_size)\n","\n","\n","# Create the model\n","model_question4 = BiLSTMForQuestion(word_vector_size, hidden_size).to(device)\n","model_document4 = BiLSTMForDocument(document_word_vector_size, hidden_size, len(token_labels), num_layers = 1, attention_method = \"Dot Product\").to(device)\n","\n","# Train the model on training data - using dot product for attention\n","trainIters(model_document4, model_question4, no_of_epoch, print_every=100, learning_rate=learning_rate_val)\n","\n","# Evaluate the model on test data\n","model_document4.eval()\n","model_question4.eval()\n","precision, recall, f1 = evaluate_model(model_document4, model_question4)\n","\n","performance_df.loc[3] = ['Attention Ablation Study', 'FastText only. Dot Product Attention', precision, recall, f1 ]"],"metadata":{"id":"CfRK-BeiNSVi","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e2df1e9b-c0e9-4e35-9449-ed75c7f107ae"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0m 46s (- 7m 2s) (100 10%) 1.2277\n","1m 31s (- 6m 7s) (200 20%) 1.0611\n","2m 21s (- 5m 29s) (300 30%) 0.9798\n","3m 11s (- 4m 46s) (400 40%) 0.9774\n","3m 54s (- 3m 54s) (500 50%) 0.9902\n","4m 42s (- 3m 8s) (600 60%) 0.9926\n","5m 28s (- 2m 20s) (700 70%) 1.0275\n","6m 19s (- 1m 34s) (800 80%) 0.9608\n","7m 5s (- 0m 47s) (900 90%) 0.9763\n","7m 51s (- 0m 0s) (1000 100%) 1.0016\n","\n","Below is the performance metrics of this model on test data:\n","Precision: 0.1723404255319149\n","Recall: 0.3360995850622407\n","F1 score: 0.2278481012658228\n"]}]},{"cell_type":"markdown","source":["We continue with the same word embeddings and create a model with a BiLSTM for question processing and a BiLSTM with scaled dot product attention for document processing. The model is trained and evaluated on the test data, and the performance metrics are recorded in performance_df. This variant investigates the impact of using **scaled dot product attention**."],"metadata":{"id":"MFxh8aSkVHJk"}},{"cell_type":"code","source":["# We can use the previous encoded variables as we won't change word embeddings now\n","\n","# Create the model\n","model_question5 = BiLSTMForQuestion(word_vector_size, hidden_size).to(device)\n","model_document5 = BiLSTMForDocument(document_word_vector_size, hidden_size, len(token_labels), num_layers = 1, attention_method = \"Scaled Dot Product\").to(device)\n","\n","# Train the model on training data - using scaled dot product for attention\n","trainIters(model_document5, model_question5, no_of_epoch, print_every=100, learning_rate=learning_rate_val)\n","\n","# Evaluate the model on test data\n","model_document5.eval()\n","model_question5.eval()\n","precision, recall, f1 = evaluate_model(model_document5, model_question5)\n","\n","performance_df.loc[4] = ['Attention Ablation Study', 'FastText only. Scaled Dot Product Attention', precision, recall, f1 ]"],"metadata":{"id":"E0VAR8GF9hSD","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1e47b07d-f09d-468d-e692-c7996bd1ebb6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0m 49s (- 7m 27s) (100 10%) 1.2618\n","1m 37s (- 6m 28s) (200 20%) 1.1771\n","2m 24s (- 5m 38s) (300 30%) 1.0007\n","3m 16s (- 4m 55s) (400 40%) 0.9803\n","4m 2s (- 4m 2s) (500 50%) 0.9917\n","4m 52s (- 3m 15s) (600 60%) 0.9938\n","5m 42s (- 2m 26s) (700 70%) 1.0282\n","6m 36s (- 1m 39s) (800 80%) 0.9614\n","7m 25s (- 0m 49s) (900 90%) 0.9772\n","8m 14s (- 0m 0s) (1000 100%) 1.0026\n","\n","Below is the performance metrics of this model on test data:\n","Precision: 0.16523605150214593\n","Recall: 0.31950207468879666\n","F1 score: 0.21782178217821782\n"]}]},{"cell_type":"markdown","source":["We use the previous encoded variables and create a model with a BiLSTM for question processing and a BiLSTM with cosine similarity attention for document processing. The model is trained and evaluated on the test data, and the performance metrics are recorded in performance_df. This variant explores the impact of using **cosine similarity attention**."],"metadata":{"id":"Ag9cU51GVNP4"}},{"cell_type":"code","source":["# We can use the previous encoded variables as we won't change word embeddings now\n","\n","# Create the model\n","model_question6 = BiLSTMForQuestion(word_vector_size, hidden_size).to(device)\n","model_document6 = BiLSTMForDocument(document_word_vector_size, hidden_size, len(token_labels), num_layers = 1, attention_method = \"Cosine Similarity\").to(device)\n","\n","# Train the model on training data - using cosine for attention\n","trainIters(model_document6, model_question6, no_of_epoch, print_every=100, learning_rate=learning_rate_val)\n","\n","# Evaluate the model on test data\n","model_document6.eval()\n","model_question6.eval()\n","precision, recall, f1 = evaluate_model(model_document6, model_question6)\n","\n","performance_df.loc[5] = ['Attention Ablation Study', 'FastText only. Cosine Similarity Attention', precision, recall, f1 ]"],"metadata":{"id":"KioekTI-FqwB","colab":{"base_uri":"https://localhost:8080/"},"outputId":"494ec2b0-e719-42ca-ab69-df86124d0432"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0m 58s (- 8m 44s) (100 10%) 1.2460\n","1m 53s (- 7m 34s) (200 20%) 1.1351\n","2m 50s (- 6m 38s) (300 30%) 0.9918\n","3m 51s (- 5m 47s) (400 40%) 0.9808\n","4m 45s (- 4m 45s) (500 50%) 0.9921\n","5m 43s (- 3m 48s) (600 60%) 0.9940\n","6m 41s (- 2m 52s) (700 70%) 1.0283\n","7m 43s (- 1m 55s) (800 80%) 0.9616\n","8m 41s (- 0m 57s) (900 90%) 0.9773\n","9m 38s (- 0m 0s) (1000 100%) 1.0026\n","\n","Below is the performance metrics of this model on test data:\n","Precision: 0.1670235546038544\n","Recall: 0.3236514522821577\n","F1 score: 0.22033898305084748\n"]}]},{"cell_type":"markdown","source":["###3.3. Hyper Parameter Testing"],"metadata":{"id":"llzGjUe6NDnB"}},{"cell_type":"markdown","source":["we created a model with a BiLSTM for question processing and a BiLSTM for document processing. The model is trained with hyperparameters set to **500 epochs and a learning rate of 0.01**. It is then evaluated on the test data, and the performance metrics are recorded in **performance_df**. This variant explores the impact of different hyperparameters."],"metadata":{"id":"-jPfFs8YVRnI"}},{"cell_type":"code","source":["# We can use the previous encoded variables as we won't change word embeddings now\n","\n","# Create the model\n","model_question7 = BiLSTMForQuestion(word_vector_size, hidden_size).to(device)\n","model_document7 = BiLSTMForDocument(document_word_vector_size, hidden_size, len(token_labels)).to(device)\n","\n","no_of_epoch = 500\n","learning_rate_val = 0.01\n","# Train the model on training data\n","trainIters(model_document7, model_question7, no_of_epoch, print_every=100, learning_rate=learning_rate_val)\n","\n","# Evaluate the model on test data\n","model_document7.eval()\n","model_question7.eval()\n","precision, recall, f1 = evaluate_model(model_document7, model_question7)\n","\n","performance_df.loc[6] = ['Hyper Parameter Testing', 'FastText only. 500 Epochs, 0.01 Learning Rate', precision, recall, f1 ]"],"metadata":{"id":"2Xj4PNyrNDBH","colab":{"base_uri":"https://localhost:8080/"},"outputId":"caa41e30-cc1f-4b97-db46-1161d22f55fc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0m 50s (- 3m 23s) (100 20%) 1.0620\n","1m 40s (- 2m 30s) (200 40%) 1.0262\n","2m 28s (- 1m 39s) (300 60%) 0.9779\n","3m 21s (- 0m 50s) (400 80%) 0.9759\n","4m 7s (- 0m 0s) (500 100%) 0.9890\n","\n","Below is the performance metrics of this model on test data:\n","Precision: 0.1979381443298969\n","Recall: 0.3983402489626556\n","F1 score: 0.2644628099173554\n"]}]},{"cell_type":"markdown","source":[" Training with **1000 Epochs and 0.001 Learning Rate** (Fast Text word representation only, Dot Product Attention)"],"metadata":{"id":"jC613Ui1VX8G"}},{"cell_type":"code","source":["# Create the model\n","model_question8 = BiLSTMForQuestion(word_vector_size, hidden_size).to(device)\n","model_document8 = BiLSTMForDocument(document_word_vector_size, hidden_size, len(token_labels)).to(device)\n","\n","no_of_epoch = 1000\n","learning_rate_val = 0.001\n","# Train the model on training data\n","trainIters(model_document8, model_question8, no_of_epoch, print_every=100, learning_rate=learning_rate_val)\n","\n","# Evaluate the model on test data\n","model_document8.eval()\n","model_question8.eval()\n","precision, recall, f1 = evaluate_model(model_document8, model_question8)\n","\n","performance_df.loc[7] = ['Hyper Parameter Testing', 'FastText only. 1000 Epochs, 0.001 Learning Rate', precision, recall, f1 ]"],"metadata":{"id":"ytIMeSLABIvV","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a3a52b25-4c51-4de4-c843-7160961ed9c2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0m 48s (- 7m 19s) (100 10%) 1.2402\n","1m 35s (- 6m 20s) (200 20%) 1.0606\n","2m 22s (- 5m 33s) (300 30%) 0.9798\n","3m 13s (- 4m 50s) (400 40%) 0.9774\n","3m 59s (- 3m 59s) (500 50%) 0.9902\n","4m 47s (- 3m 11s) (600 60%) 0.9926\n","5m 36s (- 2m 24s) (700 70%) 1.0275\n","6m 28s (- 1m 37s) (800 80%) 0.9608\n","7m 16s (- 0m 48s) (900 90%) 0.9763\n","8m 3s (- 0m 0s) (1000 100%) 1.0016\n","\n","Below is the performance metrics of this model on test data:\n","Precision: 0.18277310924369747\n","Recall: 0.36099585062240663\n","F1 score: 0.24267782426778245\n"]}]},{"cell_type":"markdown","source":["Training with **1000 Epochs and 0.1 Learning Rate** (Fast Text word representation only, Dot Product Attention)"],"metadata":{"id":"tgkmsZqtVdLS"}},{"cell_type":"code","source":["# Create the model\n","model_question9 = BiLSTMForQuestion(word_vector_size, hidden_size).to(device)\n","model_document9 = BiLSTMForDocument(document_word_vector_size, hidden_size, len(token_labels)).to(device)\n","\n","no_of_epoch = 1000\n","learning_rate_val = 0.1\n","# Train the model on training data\n","trainIters(model_document9, model_question9, no_of_epoch, print_every=1000, learning_rate=learning_rate_val)\n","\n","# Evaluate the model on test data\n","model_document9.eval()\n","model_question9.eval()\n","precision, recall, f1 = evaluate_model(model_document9, model_question9)\n","\n","performance_df.loc[8] = ['Hyper Parameter Testing', 'FastText only. 1000 Epochs, 0.1 Learning Rate', precision, recall, f1 ]"],"metadata":{"id":"0i3BfIXTGDlM","colab":{"base_uri":"https://localhost:8080/"},"outputId":"59fc839b-d2d2-4eaa-cce2-c0f89fe326d0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["8m 19s (- 0m 0s) (1000 100%) 0.9970\n","\n","Below is the performance metrics of this model on test data:\n","Precision: 0.25621414913957935\n","Recall: 0.5560165975103735\n","F1 score: 0.35078534031413616\n"]}]},{"cell_type":"markdown","source":[" Training with **5000 Epochs and 0.1 Learning Rate** (Fast Text word representation only, Dot Product Attention)"],"metadata":{"id":"-ct7PYGEy5TP"}},{"cell_type":"code","source":["# Create the model\n","model_question10 = BiLSTMForQuestion(word_vector_size, hidden_size).to(device)\n","model_document10 = BiLSTMForDocument(document_word_vector_size, hidden_size, len(token_labels)).to(device)\n","\n","no_of_epoch = 5000\n","learning_rate_val = 0.1\n","# Train the model on training data\n","trainIters(model_document10, model_question10, no_of_epoch, print_every=1000, learning_rate=learning_rate_val)\n","\n","# Evaluate the model on test data\n","model_document10.eval()\n","model_question10.eval()\n","precision, recall, f1 = evaluate_model(model_document10, model_question10)\n","\n","performance_df.loc[9] = ['Hyper Parameter Testing', 'FastText only. 5000 Epochs, 0.1 Learning Rate', precision, recall, f1 ]"],"metadata":{"id":"a9EFFkVMGDwd","colab":{"base_uri":"https://localhost:8080/"},"outputId":"5bcd4e31-a84b-40b6-9c67-89a91b8c58b3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["7m 58s (- 31m 55s) (1000 20%) 0.9967\n","15m 58s (- 23m 57s) (2000 40%) 1.0025\n","23m 56s (- 15m 57s) (3000 60%) 1.0110\n","31m 46s (- 7m 56s) (4000 80%) 0.9879\n","39m 42s (- 0m 0s) (5000 100%) 0.9939\n","\n","Below is the performance metrics of this model on test data:\n","Precision: 0.2646502835538752\n","Recall: 0.5809128630705395\n","F1 score: 0.3636363636363636\n"]}]},{"cell_type":"markdown","source":[" Training with **10000 Epochs and 0.1 Learning Rate** (Fast Text word representation only, Dot Product Attention)"],"metadata":{"id":"X-JgxMMBy92u"}},{"cell_type":"code","source":["# Create the model\n","model_question11 = BiLSTMForQuestion(word_vector_size, hidden_size).to(device)\n","model_document11 = BiLSTMForDocument(document_word_vector_size, hidden_size, len(token_labels)).to(device)\n","\n","no_of_epoch = 10000\n","learning_rate_val = 0.1\n","# Train the model on training data\n","trainIters(model_document11, model_question11, no_of_epoch, print_every=1000, learning_rate=learning_rate_val)\n","\n","# Evaluate the model on test data\n","model_document11.eval()\n","model_question11.eval()\n","precision, recall, f1 = evaluate_model(model_document11, model_question11)\n","\n","performance_df.loc[10] = ['Hyper Parameter Testing', 'FastText only. 10000 Epochs, 0.1 Learning Rate', precision, recall, f1 ]"],"metadata":{"id":"b0B7sV7fBNdY","colab":{"base_uri":"https://localhost:8080/"},"outputId":"6e7ae85a-447e-4246-a5ed-befdc0c1917d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["8m 7s (- 73m 5s) (1000 10%) 1.1147\n","16m 7s (- 64m 30s) (2000 20%) 1.2572\n","24m 9s (- 56m 22s) (3000 30%) 1.2620\n","32m 4s (- 48m 6s) (4000 40%) 1.2628\n","40m 9s (- 40m 9s) (5000 50%) 1.2548\n","48m 22s (- 32m 14s) (6000 60%) 1.2657\n","56m 35s (- 24m 15s) (7000 70%) 1.2731\n","64m 32s (- 16m 8s) (8000 80%) 1.2753\n","72m 40s (- 8m 4s) (9000 90%) 1.2606\n","80m 39s (- 0m 0s) (10000 100%) 1.2677\n","\n","Below is the performance metrics of this model on test data:\n","Precision: 0.2660377358490566\n","Recall: 0.5850622406639004\n","F1 score: 0.36575875486381326\n"]}]},{"cell_type":"markdown","source":["We also checked the performance of document model created using **5** BiLSTM layers but it was comparable to the model created using single BiLSTM layer and  trained with 10000 epochs and 0.1 as learning rate. So, we decided to continue using the model with single BiLSTM layer as it will save us on the extra computational resources required for the extra BiLSTM layers."],"metadata":{"id":"9Q0yow8JY0yL"}},{"cell_type":"code","source":["# Create the model with 5 BiLSTM layers\n","model_question12 = BiLSTMForQuestion(word_vector_size, hidden_size).to(device)\n","model_document12 = BiLSTMForDocument(document_word_vector_size, hidden_size, len(token_labels), num_layers = 5).to(device)\n","\n","no_of_epoch = 5000\n","learning_rate_val = 0.1\n","# Train the model on training data\n","trainIters(model_document12, model_question12, no_of_epoch, print_every=1000, learning_rate=learning_rate_val)\n","\n","# Evaluate the model on test data\n","model_document12.eval()\n","model_question12.eval()\n","precision, recall, f1 = evaluate_model(model_document12, model_question12)\n","performance_df.loc[11] = ['Number of Layers - 5', 'FastText only. 5000 Epochs, 0.1 Learning Rate', precision, recall, f1 ]"],"metadata":{"id":"PB_jImihYwz1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Performance Evaluation Table of all 12 Model Variants**\n","\n","We are now creating a table for all the 12 types of model with Precision Score , Recall Score and F1 Score."],"metadata":{"id":"s5BQCSrxX38m"}},{"cell_type":"code","source":["from tabulate import tabulate\n","\n","# Print the DataFrame using tabulate\n","print(tabulate(performance_df, headers='keys', tablefmt='psql'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GVv60qaL0DvS","outputId":"75a942f1-2d5a-4258-f41b-1988707db66d","executionInfo":{"status":"ok","timestamp":1684414958858,"user_tz":-480,"elapsed":5,"user":{"displayName":"Abhishek Anand","userId":"00863669191482302636"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+----+--------------------------------+-------------------------------------------------+-------------------+----------------+------------+\n","|    | Model Type                     | Model Description                               |   Precision Score |   Recall Score |   F1 Score |\n","|----+--------------------------------+-------------------------------------------------+-------------------+----------------+------------|\n","|  0 | Input Embedding Ablation Study | FastText only. No TFIDF, POS and NER tags       |          0.175847 |       0.344398 |   0.232819 |\n","|  1 | Input Embedding Ablation Study | FastText with TFIDF and POS tags. No NER tag    |          0.167024 |       0.323651 |   0.220339 |\n","|  2 | Input Embedding Ablation Study | FastText with TFIDF, POS and NER tags           |          0.148796 |       0.282158 |   0.194842 |\n","|  3 | Attention Ablation Study       | FastText only. Dot Product Attention            |          0.17234  |       0.3361   |   0.227848 |\n","|  4 | Attention Ablation Study       | FastText only. Scaled Dot Product Attention     |          0.165236 |       0.319502 |   0.217822 |\n","|  5 | Attention Ablation Study       | FastText only. Cosine Similarity Attention      |          0.167024 |       0.323651 |   0.220339 |\n","|  6 | Hyper Parameter Testing        | FastText only. 500 Epochs, 0.01 Learning Rate   |          0.197938 |       0.39834  |   0.264463 |\n","|  7 | Hyper Parameter Testing        | FastText only. 1000 Epochs, 0.001 Learning Rate |          0.182773 |       0.360996 |   0.242678 |\n","|  8 | Hyper Parameter Testing        | FastText only. 1000 Epochs, 0.1 Learning Rate   |          0.256214 |       0.556017 |   0.350785 |\n","|  9 | Hyper Parameter Testing        | FastText only. 5000 Epochs, 0.1 Learning Rate   |          0.26465  |       0.580913 |   0.363636 |\n","| 10 | Hyper Parameter Testing        | FastText only. 10000 Epochs, 0.1 Learning Rate  |          0.266038 |       0.585062 |   0.365759 |\n","| 11 | Number of Layers - 5           | FastText only. 5000 Epochs, 0.1 Learning Rate   |          0.266038 |       0.585062 |   0.365759 |\n","+----+--------------------------------+-------------------------------------------------+-------------------+----------------+------------+\n"]}]},{"cell_type":"markdown","source":["### Save the best model\n","\n","We find the 11th variant of model (Fast Text word representation only, Dot Product Attention and training with 10000 epochs and 0.1 as learning rate) as the best performing model. So, we save that model."],"metadata":{"id":"zb1vqVlweMZV"}},{"cell_type":"code","source":["# Save the best performing model\n","torch.save(model_document11,'document_model.pt')\n","torch.save(model_question11,'question_model.pt')"],"metadata":{"id":"G-1KAH967G2W"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Loading the **Document** model."],"metadata":{"id":"dXApcVwjX_-U"}},{"cell_type":"code","source":["model_document = torch.load('document_model.pt')\n","model_document.eval()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I_lD26duDL6C","outputId":"9025a2a2-224b-4a5e-e5cc-b6b05295d237"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BiLSTMForDocument(\n","  (lstm): LSTM(50, 50, batch_first=True, bidirectional=True)\n","  (fc): Linear(in_features=100, out_features=4, bias=True)\n",")"]},"metadata":{},"execution_count":61}]},{"cell_type":"markdown","source":["Loading the **Question** model."],"metadata":{"id":"dW_qKUEDYDwS"}},{"cell_type":"code","source":["question_model = torch.load('question_model.pt')\n","question_model.eval()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u015qAq-Dc-K","outputId":"9c9d4ee6-3e01-4b0e-e64e-106d603714b4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BiLSTMForQuestion(\n","  (lstm): LSTM(50, 50, batch_first=True, bidirectional=True)\n",")"]},"metadata":{},"execution_count":62}]},{"cell_type":"markdown","source":["Model Testing of our best model on the entire test data."],"metadata":{"id":"F4Pb00RqehxS"}},{"cell_type":"code","source":["precision, recall, f1 = evaluate_model(model_document, question_model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X6a4KXW0DoPC","outputId":"0ab6ef42-fd25-4ff0-fbf4-fb46341a9058"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Below is the performance metrics of this model on test data:\n","Precision: 0.2660377358490566\n","Recall: 0.5850622406639004\n","F1 score: 0.36575875486381326\n"]}]}]}